{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "24b6ff0a",
   "metadata": {},
   "source": [
    "# Understanding TF-IDF\n",
    "\n",
    "In this section we will work with texts and derive weighted metrics based on words (or terms) frequencies within these texts. More precisely, we will look at the _TF-IDF_ metric, which stands for _Term Frequency-Inverse Document Frequency_, to produce our metrics which will allow us to measure and evaluate how important certain words are in documents that are part of our IMDb corpus. The \"texts\" or \"documents\" we will look at are 'plot' descriptions in the IMDb dataset. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "137dce50",
   "metadata": {},
   "source": [
    "## Loading the IMDb dataset\n",
    "\n",
    "Load the IMDb dataset and look closely at the 'Plot' column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a671c300",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "df_imdb = pd.read_csv('../data/imdb.csv', sep=',')\n",
    "\n",
    "#df_imdb['Plot']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37cca05d",
   "metadata": {},
   "source": [
    "## Create a data structure\n",
    "\n",
    "We need a custom data structure to carry out our TF-IDF calculations. Create a python dictionary having for keys the indices of the dataframe above and for value another dictionary with 'plot' as an entry for each row in the dataframe. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ae21cc26",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_dict = {}\n",
    "for index, plot in df_imdb['Plot'].items():\n",
    "    \n",
    "    if type(plot) != str:\n",
    "        continue\n",
    "    \n",
    "    plot_dict[index] = {'plot': plot}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d0fff1e",
   "metadata": {},
   "source": [
    "## Tokenize and filter\n",
    "\n",
    "Now that we have the plot of each IMDb entry in our dictionary, it is time to tokenize each plot's text and clean it up. Do we need punctuations as part of our tokens? Are there \"stop words\" we could get rid off? Please complete the following tokenizer function utilising the spacy library (which you used in Data Mining - also, remember to uncomment the first line if you are using spacy for the first time). When this is done, augment your custom dictionary with the plot's tokens for each entry."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "4a9c79d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !python3 -m spacy download en_core_web_sm\n",
    "\n",
    "import spacy\n",
    "\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "\n",
    "def split_and_stop(text):\n",
    "    \n",
    "    # tokenize the text with spacy\n",
    "    tokens = nlp(text.lower())\n",
    "    \n",
    "    return [token.text for token in tokens if not token.is_punct and not token.is_stop]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1534bec3",
   "metadata": {},
   "outputs": [],
   "source": [
    "for index, v in plot_dict.items():\n",
    "    \n",
    "    v['tokens'] = split_and_stop(v['plot'])  \n",
    "    \n",
    "    #this may take some time... so we are going to print something to keep up with progress\n",
    "    print('.', end='')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "9934aeb1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# for k in list(plot_dict.keys())[:5]:\n",
    "#     print(\"------\")\n",
    "#     print(plot_dict[k]['plot'])\n",
    "#     print(plot_dict[k]['tokens'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e13c9b1",
   "metadata": {},
   "source": [
    "## Understanding Term Frequency (TF)\n",
    "\n",
    "$$\n",
    "tf(t, d) = \\frac{n_{t}} {\\sum_{k} n_{k}}\n",
    "$$\n",
    "\n",
    "_Term Frequency_ is a normalised metric that measures how frequent a certain term $t$ is in a given document $d$. In the formula above ${n_{t}}$ stands for the number of times the term $t$ occur in document $d$ while $\\sum_{k} n_{k}$ is the sum of all terms in the document (its length in other words). Note that term $t$ can potentially occur many times in $d$ hence the need to normalise the metric over the sum of all terms. Below is a function definition `calculate_tf` which takes as input the `tokens` of a certain document $d$ and counts the number of occurences of each terms in the document and calculate their normalised frequency. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "102a1dde",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_tf(tokens):\n",
    "    unique_tokens = set(tokens)\n",
    "    term_count = dict.fromkeys(unique_tokens, 0)\n",
    "    term_frequency = dict.fromkeys(unique_tokens, 0)\n",
    "    N = float(len(tokens))\n",
    "    for term in tokens:\n",
    "        term_count[term] += 1\n",
    "        term_frequency[term] += 1 / N\n",
    "    return term_count, term_frequency        "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b666e49",
   "metadata": {},
   "source": [
    "Considering the function `calculate_tf` above, augment your custom dictionary with both the `term_count` and normalised `term_frequency` given the respective plot's `tokens` you previously computed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "c1178b35",
   "metadata": {},
   "outputs": [],
   "source": [
    "for index, v in plot_dict.items():\n",
    "    \n",
    "    t_count, t_frequency = calculate_tf(v['tokens'])\n",
    "    \n",
    "    v['count'] = t_count\n",
    "    v['tf'] = t_frequency"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "d7e8221f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# for k in list(plot_dict.keys())[:5]:\n",
    "#     print(\"------\")\n",
    "#     print(plot_dict[k])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f4fba93",
   "metadata": {},
   "source": [
    "## Understanding Inverse Document Frequency (IDF)\n",
    "\n",
    "$$\n",
    "idf(t, D) = \\log\\frac{|D|}{|{d_{i} \\in D : t \\in d_{i}}|}\n",
    "$$\n",
    "\n",
    "_Inverse Document Frequency_ is a metric that measures of important a term $t$ is in a given corpus (or collection) $D$ of documents $d_{i}$. While _Term Frequency_ measures the frequency of a term $t$ in a single document $d$, here _IDF_ consider frequency of a term $t$ over the whole corpus $D$ as to derive a weight on the statistical significance of term $t$ overall. The idea here is that common words which occur in many documents (\"man\" or a stop word like \"it\" for example) hold little importance overall as they are redundant. What _IDF_ does is to give more weight to words that are uncommon overall yet possibly significant for certain documents. This is the reason why the metric takes the $\\log$ of the fraction $\\frac{|D|}{|{d_{i} \\in D : t \\in d_{i}}|}$ where $|D|$ is the number of documents in corpus $D$ and $|{d_{i} \\in D : t \\in d_{i}}|$ is the number of times a term $t$ appears in a document in the corpus. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "86ef85b2",
   "metadata": {},
   "source": [
    "The first thing we need to do to calculate _IDF_ is to establish the overall vocabulary of the entire corpus. What are all the unique words (or terms) in all of our plots? How many unique words do we have? Consider the following `bag_of_words` python set and fill it with all the unique terms present in our plots. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "65c22bdf",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "14549"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Vocabulary -> bag of words\n",
    "\n",
    "bag_of_words = set()\n",
    "\n",
    "for index, v in plot_dict.items():    \n",
    "    \n",
    "    bag_of_words = bag_of_words.union(set(v['tokens']))\n",
    "    \n",
    "len(bag_of_words)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "860d1f84",
   "metadata": {},
   "source": [
    "Now, remember we calculated a `term_count` for each term in each document when we calculated the _TF_ with `calculate_tf` above? We need to use this pre-calculated informatin here to derive $|{d_{i} \\in D : t \\in d_{i}}|$ which is the number of times a term $t$ appears in a document in the corpus. Make a list of each `term_count` you recorded in your custom dicitonary as to use it to computer _IDF_ below. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "cdbc91b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "list_all_documents_count = [v['count'] for index, v in plot_dict.items()]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a3b41e3",
   "metadata": {},
   "source": [
    "Here is function defintion `calculate_idf` that computes the _IDF_ of all the terms in our corpus. It takes a list of `term_count` as `documents_count_list` and a overall vocabulary as `bag_of_words`. Can you make sense of the function in light of the $idf(t, D)$ formula above?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "0d8035ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "\n",
    "def calculate_idf(documents_count_list, bag_of_words):\n",
    "    \n",
    "    idf = dict.fromkeys(bag_of_words, 0)\n",
    "    D = len(documents_count_list)\n",
    "    \n",
    "    for d in documents_count_list:\n",
    "        for term, count in d.items():\n",
    "            if count > 0:\n",
    "                idf[term] += 1\n",
    "                \n",
    "    for term, document_count in idf.items():\n",
    "        idf[term] = math.log(D / float(document_count))\n",
    "        \n",
    "    return idf"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b7b56c16",
   "metadata": {},
   "source": [
    "Lets calculate the _IDF_ then using the function above. What are the highest weight? What are the lowest weight?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8a59006",
   "metadata": {},
   "outputs": [],
   "source": [
    "idf = calculate_idf(list_all_documents_count, bag_of_words)\n",
    "\n",
    "# helper to visualise the terms in the IDF , sorted according to their score\n",
    "\n",
    "sorted_idf = {k: v for k, v in sorted(idf.items(), key=lambda item: item[1])}\n",
    "# sorted_idf = {k: v for k, v in sorted(idf.items(), key=lambda item: item[1], reverse=True)}\n",
    "for k in list(sorted_idf.keys())[:50]:\n",
    "    print(f'{k} - {sorted_idf[k]}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "96e7fbb1",
   "metadata": {},
   "source": [
    "## Putting it together: TF-IDF\n",
    "\n",
    "$$\n",
    "tf-idf(t, d, D) = tf(t, d) \\cdot idf(t, D)\n",
    "$$\n",
    "\n",
    "Putting _TF_ and _IDF_ together is quite simple. Since _IDF_ is a weight for each term in the corpus, simply multiply the terms' weight value to all the _TF_ we already have calculated. Here is a function `calculate_tf_idf` that does just that!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "3439930f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_tf_idf(tf, idf):\n",
    "    tf_idf = dict.fromkeys(tf.keys(), 0)\n",
    "    for term, frequency in tf.items():\n",
    "        tf_idf[term] = frequency * idf[term]\n",
    "    return tf_idf    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d782373",
   "metadata": {},
   "source": [
    "With the function above, calculate the _TF-IDF_ of all plots in your custom dictionary and record the results in the dictionary itself. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "f6f7b579",
   "metadata": {},
   "outputs": [],
   "source": [
    "for index, v in plot_dict.items(): \n",
    "    tf_idf = calculate_tf_idf(v['tf'], idf)\n",
    "    v['tf_idf'] = tf_idf"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad59e5c3",
   "metadata": {},
   "source": [
    "What is the difference between _TF_ and _IDF_ for a given plot?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7febe12",
   "metadata": {},
   "outputs": [],
   "source": [
    "for k in list(plot_dict.keys())[:5]:\n",
    "    print(\"------\")\n",
    "    print(plot_dict[k]['tf'])\n",
    "    print(plot_dict[k]['tf_idf'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4378e90b",
   "metadata": {},
   "source": [
    "## Save the data\n",
    "\n",
    "Save your custom dictionary you have constructed above in a json file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "7bff1296",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "with open('data/IFIDF_IMDb_plots.json', 'w') as fp:\n",
    "    json.dump(plot_dict, fp, indent=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05141eb2",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
