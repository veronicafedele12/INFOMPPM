{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ðŸ“š Recommendations based on Frequently Reviewed Together (association rules)\n",
    "For the final segment of this assignment, refer to section 5.4 of the _Practical Recommender Systems_ book (pages 113-127). After reading, download the code provided by the book and focus on the `association_rules_calculator.py` in the `builder` directory. Your task is to adapt this code for use in this notebook, translating its steps into a format suitable for our environment. Here's a simplified outline based on the source code:\n",
    "\n",
    "The steps found in the source code are:\n",
    "1. Load the data\n",
    "2. Generate transactions or, in our case reviews\n",
    "3. Calculate the Support Confidence\n",
    "4. Save the results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Load the data\n",
    "Instead of using a database, load your `.csv` files into a dataframe. Select the data necessary for identifying which user reviewed which books.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "df = pd.read_csv('data/BX-Book-Ratings-Subset.csv', sep=';', encoding='latin-1')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_users = df.groupby('User-ID')['ISBN'].count().reset_index(name='counts')\n",
    "users = list(df_users[(df_users['counts'] > 100) & (df_users['counts'] < 200)]['User-ID'])\n",
    "\n",
    "df.loc[df['User-ID'].isin(users)]\n",
    "\n",
    "#df[df['User-ID'] == 104636]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Generating the reviews\n",
    "In this context, transactions are the reviews. You need to compile a list of lists, where each inner list contains reviews that are related, similar to how shopping lists are grouped in the example: `[['eggs','milk','bread'], ['bacon', 'bread'], [...], [...]]`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_reviews = df.groupby('User-ID')['ISBN'].apply(list)\n",
    "reviewed = df_reviews.values.tolist()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Calculate the Support Confidence\n",
    "This requires some puzzling, but looking at the source code will give you a clear idea. You can reuse the subroutines in the source code and pass along the list containing the reviews belonging together. Play around with the _minimum support_ parameter. Too strict will result in fewer associations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# this code originated from the book Practical Recommender System. \n",
    "# Some minor tweaks to make it work with the current dataset.\n",
    "\n",
    "from collections import defaultdict\n",
    "from itertools import combinations\n",
    "from datetime import datetime\n",
    "\n",
    "def calculate_itemsets_one(reviewed, min_sup=0.01):\n",
    "    N = len(reviewed)\n",
    "    print(N)\n",
    "    temp = defaultdict(int)\n",
    "    one_itemsets = dict()\n",
    "\n",
    "    for items in reviewed:\n",
    "        for item in items:\n",
    "            inx = frozenset({item})\n",
    "            temp[inx] += 1\n",
    "\n",
    "    print(\"temp:\")\n",
    "    i = 0\n",
    "    # remove all items that is not supported.\n",
    "    for key, itemset in temp.items():\n",
    "        #print(f\"{key}, {itemset}, {min_sup}, {min_sup * N}\")\n",
    "        if itemset > min_sup * N:\n",
    "            i = i + 1\n",
    "            one_itemsets[key] = itemset\n",
    "    print(i)\n",
    "    return one_itemsets\n",
    "\n",
    "def calculate_itemsets_two(reviewed, one_itemsets):\n",
    "    two_itemsets = defaultdict(int)\n",
    "\n",
    "    for items in reviewed:\n",
    "        items = list(set(items))  # remove duplications\n",
    "\n",
    "        if (len(items) > 2):\n",
    "            for perm in combinations(items, 2):\n",
    "                if has_support(perm, one_itemsets):\n",
    "                    two_itemsets[frozenset(perm)] += 1\n",
    "        elif len(items) == 2:\n",
    "            if has_support(items, one_itemsets):\n",
    "                two_itemsets[frozenset(items)] += 1\n",
    "    return two_itemsets\n",
    "\n",
    "def calculate_association_rules(one_itemsets, two_itemsets, N):\n",
    "    timestamp = datetime.now()\n",
    "\n",
    "    rules = []\n",
    "    for source, source_freq in one_itemsets.items():\n",
    "        for key, group_freq in two_itemsets.items():\n",
    "            if source.issubset(key):\n",
    "                target = key.difference(source)\n",
    "                support = group_freq / N\n",
    "                confidence = group_freq / source_freq\n",
    "                rules.append((timestamp, next(iter(source)), next(iter(target)),\n",
    "                              confidence, support))\n",
    "    return rules\n",
    "\n",
    "def has_support(perm, one_itemsets):\n",
    "  return frozenset({perm[0]}) in one_itemsets and \\\n",
    "    frozenset({perm[1]}) in one_itemsets\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "min_sup = 0.01\n",
    "N = len(reviewed)\n",
    "\n",
    "one_itemsets = calculate_itemsets_one(reviewed, min_sup)\n",
    "two_itemsets = calculate_itemsets_two(reviewed, one_itemsets)\n",
    "rules = calculate_association_rules(one_itemsets, two_itemsets, N)\n",
    "\n",
    "# check how many associations are made\n",
    "len(rules)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. Save the results\n",
    "Create a dataframe for the results of step 3. In order to make it work with the current app please make sure the columns are `source;target;support;confidence`. Save the recommendations as `recommendations-seeded-associations.csv` and replace the file in the app directory."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "associations = []\n",
    "\n",
    "# iterate through results and create data structure containing the results\n",
    "for rule in rules:\n",
    "  association = {\n",
    "    'source':str(rule[1]),\n",
    "    'target':str(rule[2]),\n",
    "    'confidence':rule[3],\n",
    "    'support':rule[4]\n",
    "  }\n",
    "  # append to list\n",
    "  associations.append(association)\n",
    "\n",
    "# create dataframe\n",
    "df_associations = pd.DataFrame(associations) \n",
    "\n",
    "df_associations.to_csv('recommendations-seeded-associations.csv', index=False, sep=';')"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "37c10f95d263926787ebf1d430d11186fc6b9bac835b8518e0b5006ed24f0c36"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
